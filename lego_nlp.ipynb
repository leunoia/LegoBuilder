{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku"
      ],
      "metadata": {
        "id": "jkEijWJ3yl4-"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lego = pd.read_csv('/content/lego_data_clean_translated.csv')\n",
        "lego.head()\n",
        "\n",
        "\n",
        "toy_name_en = lego['toy_name_en'].values\n",
        "print(toy_name_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5T3C7EUJebh",
        "outputId": "b27f0d5b-e650-45b9-c8c4-51ea187121be"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Himeji Castle' 'New York City' 'London' ... 'Easter Bunny House'\n",
            " 'Mighty Micros: Supergirl™ vs. Brainiac™'\n",
            " 'Mighty Micros: Batman™ vs. Harley Quinn™']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(txt):\n",
        "    txt = \"\".join(t for t in txt if t not in string.punctuation).lower()\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\", 'ignore')\n",
        "    return txt\n",
        "\n",
        "toy_name_en_clean = [clean_text(x) for x in toy_name_en]\n",
        "toy_name_en_clean[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpX1CsOnyfuN",
        "outputId": "bfcf3882-5f74-40c7-84c4-a41501dba5e3"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['himeji castle',\n",
              " 'new york city',\n",
              " 'london',\n",
              " 'paris',\n",
              " 'great pyramid of giza',\n",
              " 'taj mahal',\n",
              " 'singapore',\n",
              " 'statue of liberty',\n",
              " 'the white house',\n",
              " 'batcave shadow box']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "def get_sequence_of_tokens(corpus):\n",
        "    ## tokenization\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "    ## convert data to a token sequence\n",
        "    input_sequences = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences, total_words\n",
        "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
        "inp_sequences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HljyfnzTy8ok",
        "outputId": "cfdd126f-2b45-437f-963f-4420a7e493c3"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[697, 8],\n",
              " [67, 367],\n",
              " [67, 367, 27],\n",
              " [58, 698],\n",
              " [58, 698, 4],\n",
              " [58, 698, 4, 699],\n",
              " [700, 701],\n",
              " [703, 4],\n",
              " [703, 4, 369],\n",
              " [2, 170]]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_padded_sequences(input_sequences):\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "    label = ku.to_categorical(label, num_classes=total_words)\n",
        "    return predictors, label, max_sequence_len\n",
        "\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)\n",
        "\n",
        "print(predictors)\n",
        "print(label)\n",
        "print(max_sequence_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCr4iv7jzyrK",
        "outputId": "01acba92-73c5-4fe8-cdb7-ac95879b9348"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0 ...   0   0 697]\n",
            " [  0   0   0 ...   0   0  67]\n",
            " [  0   0   0 ...   0  67 367]\n",
            " ...\n",
            " [  0   0   0 ... 216 696  23]\n",
            " [  0   0   0 ... 696  23  19]\n",
            " [  0   0   0 ...  23  19 241]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "10\n"
          ]
        }
      ]
    }
  ]
}